{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import csv\n",
    "import fasttext\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "                                               TITLE  \\\n0         Pete The Cat Bedtime Blues Doll, 14.5 Inch   \n1  The New Yorker NYHM014 Refrigerator Magnet, 2 ...   \n2  The Ultimate Self-Sufficiency Handbook: A Comp...   \n3   Amway Nutrilite Kids Chewable Iron Tablets (100)   \n4  Teacher Planner Company A4 6 Lesson Academic T...   \n\n                                         DESCRIPTION  \\\n0  Pete the Cat is the coolest, most popular cat ...   \n1  The New Yorker Handsome Cello Wrapped Hard Mag...   \n2                                                NaN   \n3                                                NaN   \n4                                                NaN   \n\n                                       BULLET_POINTS           BRAND  \\\n0  [Pete the Cat Bedtime Blues plush doll,Based o...     MerryMakers   \n1  [Cat In A Tea Cup by New Yorker cover artist G...  The New Yorker   \n2                                Skyhorse Publishing          imusti   \n3  [Nutrilite Kids,Chewable Iron Tablets,Quantity...           Amway   \n4                                                NaN             NaN   \n\n   BROWSE_NODE_ID  \n0               0  \n1               1  \n2               2  \n3               3  \n4               4  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>TITLE</th>\n      <th>DESCRIPTION</th>\n      <th>BULLET_POINTS</th>\n      <th>BRAND</th>\n      <th>BROWSE_NODE_ID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Pete The Cat Bedtime Blues Doll, 14.5 Inch</td>\n      <td>Pete the Cat is the coolest, most popular cat ...</td>\n      <td>[Pete the Cat Bedtime Blues plush doll,Based o...</td>\n      <td>MerryMakers</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>The New Yorker NYHM014 Refrigerator Magnet, 2 ...</td>\n      <td>The New Yorker Handsome Cello Wrapped Hard Mag...</td>\n      <td>[Cat In A Tea Cup by New Yorker cover artist G...</td>\n      <td>The New Yorker</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>The Ultimate Self-Sufficiency Handbook: A Comp...</td>\n      <td>NaN</td>\n      <td>Skyhorse Publishing</td>\n      <td>imusti</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Amway Nutrilite Kids Chewable Iron Tablets (100)</td>\n      <td>NaN</td>\n      <td>[Nutrilite Kids,Chewable Iron Tablets,Quantity...</td>\n      <td>Amway</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Teacher Planner Company A4 6 Lesson Academic T...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('dataset/train.csv', escapechar=\"\\\\\", quoting=3)\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "   BROWSE_NODE_ID                                               TEXT\n0               0  Pete The Cat Bedtime Blues Doll, 14.5 Inch Pet...\n1               1  The New Yorker NYHM014 Refrigerator Magnet, 2 ...\n5               5  Men'S Full Sleeve Raglan T-Shirts Denim T-Shir...\n6               6  Glance Women's Wallet (Black) (LW-21) This Bla...\n7               7  Wild Animals Hungry Brain Educational Flash Ca...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>BROWSE_NODE_ID</th>\n      <th>TEXT</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Pete The Cat Bedtime Blues Doll, 14.5 Inch Pet...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>The New Yorker NYHM014 Refrigerator Magnet, 2 ...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>Men'S Full Sleeve Raglan T-Shirts Denim T-Shir...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6</td>\n      <td>Glance Women's Wallet (Black) (LW-21) This Bla...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7</td>\n      <td>Wild Animals Hungry Brain Educational Flash Ca...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "df['TEXT'] = df['TITLE']+\" \"+df['DESCRIPTION']+\" \"+df[\"BULLET_POINTS\"]+\" \"+df[\"BRAND\"]\n",
    "df = df.drop(['TITLE','DESCRIPTION','BULLET_POINTS','BRAND'],axis=1)\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "training_df, test_df = train_test_split(df, test_size=0.1)\n",
    "train_df, valid_df = train_test_split(training_df, test_size=0.1)\n",
    "\n",
    "train_df.iloc[:, 1] = train_df.iloc[:, 1].apply(lambda x: '__label__' + str(x))\n",
    "valid_df.iloc[:, 1] = valid_df.iloc[:, 1].apply(lambda x: '__label__' + str(x))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "train_df[['BROWSE_NODE_ID','TEXT']].to_csv('train.txt',\n",
    "                                            index=False,\n",
    "                                            sep=' ',\n",
    "                                            header=None,\n",
    "                                            quoting=csv.QUOTE_NONE,\n",
    "                                            quotechar=\"\",\n",
    "                                            escapechar=\" \")\n",
    "\n",
    "valid_df[['BROWSE_NODE_ID','TEXT']].to_csv('valid.txt',\n",
    "                                            index=False,\n",
    "                                            sep=' ',\n",
    "                                            header=None,\n",
    "                                            quoting=csv.QUOTE_NONE,\n",
    "                                            quotechar=\"\",\n",
    "                                            escapechar=\" \")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "FASTTEXT_LABEL = '__label__'\n",
    "\n",
    "def create_text_file(input_path: str, output_path: str, encoding:str ='utf-8'):\n",
    "    with open(input_path, encoding=encoding) as f_in, open(output_path, 'w', encoding=encoding) as f_out:\n",
    "        for line in f_in:\n",
    "            try:\n",
    "                tokens = []\n",
    "                for token in line.split(\" \"):\n",
    "                    if FASTTEXT_LABEL not in token:\n",
    "                        tokens.append(token)\n",
    "                text = \" \".join(tokens)\n",
    "            except ValueError as e:\n",
    "                continue\n",
    "            f_out.write(text)\n",
    "\n",
    "create_text_file('train.txt','text-only-train.txt')\n",
    "create_text_file('valid.txt','text-only-valid.txt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "!cat text-only-train.txt text-only-valid.txt > raw-text.txt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "tokenizer = ByteLevelBPETokenizer(lowercase=True)\n",
    "tokenizer.train(\n",
    "    'raw-text.txt',\n",
    "    vocab_size=10000,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from tokenizers.implementations import BaseTokenizer\n",
    "\n",
    "\n",
    "def tokenize_text(tokenizer: BaseTokenizer, text: str) -> str:\n",
    "    \"\"\"\n",
    "    Given the raw text, tokenize it using the trained tokenizer and\n",
    "    outputs the tokenized text.\n",
    "    \"\"\"\n",
    "    return ' '.join(tokenizer.encode(text).tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def create_tokenized_file(input_path: str, output_path: str,\n",
    "                          tokenizer: BaseTokenizer, encoding: str='utf-8'):\n",
    "    with open(input_path, encoding=encoding) as f_in, open(output_path, 'w', encoding=encoding) as f_out:\n",
    "\n",
    "        for line in f_in:\n",
    "            try:\n",
    "                # the labels remains untouched during the preprocessing step as its\n",
    "                # already in a format that fasttext can consume\n",
    "                tokens = []\n",
    "                labels = []\n",
    "                for token in line.split(' '):\n",
    "                    if FASTTEXT_LABEL in token:\n",
    "                        labels.append(token)\n",
    "                    else:\n",
    "                        tokens.append(token)\n",
    "\n",
    "                text = ' '.join(tokens)\n",
    "                label = ' '.join(labels)\n",
    "            except ValueError as e:\n",
    "                continue\n",
    "\n",
    "            tokenized_text = tokenize_text(tokenizer, text)\n",
    "            new_line = label + ' ' + tokenized_text\n",
    "            f_out.write(new_line)\n",
    "            f_out.write('\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [19]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mcreate_tokenized_file\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtrain.txt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtokenized-text-train.txt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m create_tokenized_file(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvalid.txt\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtokenized-text-valid.txt\u001B[39m\u001B[38;5;124m'\u001B[39m, tokenizer)\n",
      "Input \u001B[0;32mIn [10]\u001B[0m, in \u001B[0;36mcreate_tokenized_file\u001B[0;34m(input_path, output_path, tokenizer, encoding)\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     20\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[0;32m---> 22\u001B[0m tokenized_text \u001B[38;5;241m=\u001B[39m \u001B[43mtokenize_text\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     23\u001B[0m new_line \u001B[38;5;241m=\u001B[39m label \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m tokenized_text\n\u001B[1;32m     24\u001B[0m f_out\u001B[38;5;241m.\u001B[39mwrite(new_line)\n",
      "Input \u001B[0;32mIn [9]\u001B[0m, in \u001B[0;36mtokenize_text\u001B[0;34m(tokenizer, text)\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtokenize_text\u001B[39m(tokenizer: BaseTokenizer, text: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mstr\u001B[39m:\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;124;03m    Given the raw text, tokenize it using the trained tokenizer and\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;124;03m    outputs the tokenized text.\u001B[39;00m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m----> 9\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mtokens)\n",
      "File \u001B[0;32m~/miniforge3/envs/Amazon-ML-Challenge39/lib/python3.9/site-packages/tokenizers/implementations/base_tokenizer.py:216\u001B[0m, in \u001B[0;36mBaseTokenizer.encode\u001B[0;34m(self, sequence, pair, is_pretokenized, add_special_tokens)\u001B[0m\n\u001B[1;32m    213\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m sequence \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    214\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mencode: `sequence` can\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt be `None`\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 216\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_tokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencode\u001B[49m\u001B[43m(\u001B[49m\u001B[43msequence\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpair\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_pretokenized\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madd_special_tokens\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "create_tokenized_file('train.txt', 'tokenized-text-train.txt', tokenizer)\n",
    "create_tokenized_file('valid.txt', 'tokenized-text-valid.txt', tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 120M words\n",
      "Number of words:  9915\n",
      "Number of labels: 62694\n",
      "Progress: 100.0% words/sec/thread: 2217244 lr:  0.000000 avg.loss:  7.254069 ETA:   0h 0m 0s avg.loss:  9.242062 ETA:   0h 2m20s8.973626 ETA:   0h 2m 9s 17.9% words/sec/thread: 2221267 lr:  0.082125 avg.loss:  8.829647 ETA:   0h 2m 4s 27.9% words/sec/thread: 2220682 lr:  0.072093 avg.loss:  8.516129 ETA:   0h 1m48s 28.0% words/sec/thread: 2220738 lr:  0.071953 avg.loss:  8.513479 ETA:   0h 1m48s 50.9% words/sec/thread: 2215544 lr:  0.049096 avg.loss:  7.971142 ETA:   0h 1m14s\n"
     ]
    }
   ],
   "source": [
    "model = fasttext.train_supervised('tokenized-text-train.txt', wordNgrams = 2,loss='hs', epoch=25)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "model.save_model('fasttext-custom-tokenizer-amazon.bin')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}