{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import csv\n",
    "import fasttext\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import ml_collections\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "tqdm.pandas()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def model_config():\n",
    "    cfg_dictionary = {\n",
    "        \"data_path\": \"dataset/train.csv\",\n",
    "\n",
    "        \"corpus_file\":\"word-corpus.txt\",\n",
    "        \"model_file\":\"model.fasttext\",\n",
    "\n",
    "        \"wordNgrams\":2,\n",
    "        \"test_size\": 0.1,\n",
    "        \"validation_size\":0.2,\n",
    "        \"loss\":'hs',\n",
    "        \"epochs\": 30,\n",
    "    }\n",
    "    cfg = ml_collections.FrozenConfigDict(cfg_dictionary)\n",
    "\n",
    "    return cfg\n",
    "cfg = model_config()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def preprocess(csv_path:str):\n",
    "    \"\"\" Reads the csv, concatenates the columns and returns a dataframe \"\"\"\n",
    "    df = pd.read_csv(csv_path,escapechar=\"\\\\\", quoting=3)\n",
    "    df = df.dropna()\n",
    "    df['TEXT'] = df['TITLE']+\" \"+df['DESCRIPTION']+\" \"+df[\"BULLET_POINTS\"]+\" \"+df[\"BRAND\"]\n",
    "    df = df.drop(['TITLE','DESCRIPTION','BULLET_POINTS','BRAND'],axis=1)\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def create_splits(dataset_file:str):\n",
    "    \"\"\"\n",
    "    Creates train, validation, test split from the dataset.\n",
    "    Converts dataframe to a format that can be consumed by fasttext, by prepending __label__ to the label.\n",
    "    each row in the dataframe would now look like\n",
    "    ```\n",
    "    |__label__234 |Cat toys, small, medium, large for all cats...|\n",
    "    ```\n",
    "    `|` denotes column separator\n",
    "    \"\"\"\n",
    "    print(\"Beginning Preprocessing ...\")\n",
    "    dataframe = preprocess(dataset_file)\n",
    "    print(\"Preprocessing Done!\")\n",
    "    training_df, test_df = train_test_split(dataframe, test_size=0.1)\n",
    "    train_df, valid_df = train_test_split(training_df, test_size=0.1)\n",
    "\n",
    "    del training_df\n",
    "\n",
    "    train_df.iloc[:, 0] = train_df.iloc[:, 0].apply(lambda x: '__label__' + str(x))\n",
    "    valid_df.iloc[:, 0] = valid_df.iloc[:, 0].apply(lambda x: '__label__' + str(x))\n",
    "\n",
    "    return train_df, valid_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def tokenize_data():\n",
    "    train_df, valid_df = create_splits(cfg.data_path)\n",
    "\n",
    "    def create_word_corpus(t_df, v_df):\n",
    "        corpus_df = pd.concat([t_df[['TEXT']],v_df[['TEXT']]])\n",
    "        corpus_df.to_csv(\n",
    "                    cfg.corpus_file,\n",
    "                    index=False,\n",
    "                    sep=' ',\n",
    "                    header=None,\n",
    "                    quoting=csv.QUOTE_NONE,\n",
    "                    quotechar=\"\",\n",
    "                    escapechar=\" \")\n",
    "    print(\"Creating corpus for tokenization ....\")\n",
    "    create_word_corpus(train_df, valid_df)\n",
    "    print(\"Done creating word corpus !\")\n",
    "\n",
    "    custom_tokenizer = ByteLevelBPETokenizer(lowercase=True)\n",
    "    print(\"Training Tokenizer ...\")\n",
    "    custom_tokenizer.train(cfg.corpus_file)\n",
    "    custom_tokenizer.save('tokenizer.json')\n",
    "    print(\"Done creating tokenizer. Tokenizer saved as `tokenizer.json`\")\n",
    "\n",
    "    def tokenize(training_df, validation_df, tokenizer):\n",
    "        training_df['TOKENIZED_TEXT'] = training_df['TEXT'].progress_apply(\n",
    "            lambda text: \" \".join(tokenizer.encode(text).tokens)\n",
    "        )\n",
    "        training_df[['TOKENIZED_TEXT', 'BROWSE_NODE_ID']].to_csv(\n",
    "                                            'tokenized-train.txt',\n",
    "                                            index=False,\n",
    "                                            sep=' ',\n",
    "                                            header=None,\n",
    "                                            quoting=csv.QUOTE_NONE,\n",
    "                                            quotechar=\"\",\n",
    "                                            escapechar=\" \")\n",
    "\n",
    "        validation_df['TOKENIZED_TEXT'] = validation_df['TEXT'].progress_apply(\n",
    "            lambda text: \" \".join(tokenizer.encode(text).tokens)\n",
    "        )\n",
    "        validation_df[['TOKENIZED_TEXT', 'BROWSE_NODE_ID']].to_csv(\n",
    "                                            'tokenized-valid.txt',\n",
    "                                            index=False,\n",
    "                                            sep=' ',\n",
    "                                            header=None,\n",
    "                                            quoting=csv.QUOTE_NONE,\n",
    "                                            quotechar=\"\",\n",
    "                                            escapechar=\" \")\n",
    "    print(\"Tokenizing data ...\")\n",
    "    tokenize(train_df, valid_df, custom_tokenizer)\n",
    "    print(\"Done tokenizing data!\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1709574 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2c6d4543f83e4463a938b754902157ac"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/189953 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fac218074ac64223928e6eb0545e0dee"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "train.txt cannot be opened for training!",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[0;32mIn [10]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m tokenize_data()\n\u001B[0;32m----> 2\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mfasttext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_supervised\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtrain.txt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwordNgrams\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mcfg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwordNgrams\u001B[49m\u001B[43m,\u001B[49m\u001B[43mloss\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcfg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloss\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepoch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcfg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mepochs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m model\u001B[38;5;241m.\u001B[39msave_model(cfg\u001B[38;5;241m.\u001B[39mmodel_file)\n",
      "File \u001B[0;32m~/miniforge3/envs/Amazon-ML-Challenge39/lib/python3.9/site-packages/fasttext/FastText.py:533\u001B[0m, in \u001B[0;36mtrain_supervised\u001B[0;34m(*kargs, **kwargs)\u001B[0m\n\u001B[1;32m    531\u001B[0m a \u001B[38;5;241m=\u001B[39m _build_args(args, manually_set_args)\n\u001B[1;32m    532\u001B[0m ft \u001B[38;5;241m=\u001B[39m _FastText(args\u001B[38;5;241m=\u001B[39ma)\n\u001B[0;32m--> 533\u001B[0m \u001B[43mfasttext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mft\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43ma\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    534\u001B[0m ft\u001B[38;5;241m.\u001B[39mset_args(ft\u001B[38;5;241m.\u001B[39mf\u001B[38;5;241m.\u001B[39mgetArgs())\n\u001B[1;32m    535\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m ft\n",
      "\u001B[0;31mValueError\u001B[0m: train.txt cannot be opened for training!"
     ]
    }
   ],
   "source": [
    "tokenize_data()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 418M words\n",
      "Number of words:  29856\n",
      "Number of labels: 9521\n",
      "Progress: 100.0% words/sec/thread: 3445313 lr:  0.000000 avg.loss:  0.937019 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "model = fasttext.train_supervised('tokenized-train.txt', wordNgrams = cfg.wordNgrams,loss=cfg.loss, epoch=cfg.epochs)\n",
    "model.save_model(cfg.model_file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "(189880, 0.7365388666526227, 0.7365388666526227)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.test('tokenized-valid.txt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : loss is manually set to a specific value. It will not be automatically optimized.\n",
      "Progress: 100.0% Trials:    6 Best score:  0.659617 ETA:   0h 0m 0s\n",
      "Training again with best arguments\n",
      "Read 418M words\n",
      "Number of words:  29856\n",
      "Number of labels: 9521\n",
      "Progress: 100.0% words/sec/thread: 2907664 lr:  0.000000 avg.loss:  1.624883 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "tuned_model = fasttext.train_supervised(input='tokenized-train.txt',loss=cfg.loss, autotuneValidationFile='tokenized-valid.txt', autotuneDuration=1800)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}