{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import fasttext\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Tuple\n",
    "import pandas as pd\n",
    "import ml_collections\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"]=\"True\"\n",
    "tqdm.pandas()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Setting up Model Hyper-Parameters\n",
    "A dictionary of hyperparameters and a few helper functions are set up."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def model_config():\n",
    "    cfg_dictionary = {\n",
    "        \"data_path\": \"dataset/train.csv\",\n",
    "\n",
    "        \"corpus_file\":\"word-corpus.txt\",\n",
    "        \"model_file\":\"fasttext-model.ftz\",\n",
    "        \"tokenizer\":\"tokenizer.json\",\n",
    "\n",
    "        \"wordNgrams\":2,\n",
    "        \"test_size\": 0.1,\n",
    "        \"validation_size\":0.2,\n",
    "        \"loss\":'hs',\n",
    "        \"epochs\": 30,\n",
    "    }\n",
    "    config = ml_collections.FrozenConfigDict(cfg_dictionary)\n",
    "\n",
    "    return config\n",
    "cfg = model_config()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def txt_saver_util(file_path:str,df:pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Saves Dataframe to text (.txt) file\n",
    "    Args:\n",
    "        file_path(str) : path to where the generated file is to be saved\n",
    "        df(pd.DataFrame) : dataframe to be saved.\n",
    "\n",
    "    \"\"\"\n",
    "    df.to_csv(file_path,\n",
    "                    index=False,\n",
    "                    sep=' ',\n",
    "                    header=False,\n",
    "                    quoting=csv.QUOTE_NONE,\n",
    "                    quotechar=\"\",\n",
    "                    escapechar=\" \")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing\n",
    "The dataset consists of multiple missing data and rows with imcomplete features. Those all missing rows are dropped. All fields `[TITLE, DESCRIPTION, BULLET_POINTS, BRAND]` are concatenated into a single field called `[TEXT]`. We conjecture that all the information about the product is necessary to make the correct browse ID classification.\n",
    "The dataset is then split into training, validation and testing sets using sci-kit learn's `train_test_split` API. all labels are prepended with `__label__` as this is the format that fasttext consumes."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def preprocess(csv_path:str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads the csv, drops missing rows and concatenates columns and returns a dataframe\n",
    "    Args:\n",
    "        csv_path(str) : path the dataset file\n",
    "\n",
    "    Returns:\n",
    "        df(pd.DataFrame) : Preprocessed dataframe\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path,escapechar=\"\\\\\", quoting=3)\n",
    "    df = df.dropna()\n",
    "    df['TEXT'] = df['TITLE']+\" \"+df['DESCRIPTION']+\" \"+df[\"BULLET_POINTS\"]+\" \"+df[\"BRAND\"]\n",
    "    df = df.drop(['TITLE','DESCRIPTION','BULLET_POINTS','BRAND'],axis=1)\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def create_splits(dataset_file:str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Creates train, validation, test split from the dataset.\n",
    "    Converts dataframe to a format that can be consumed by fasttext, by prepending __label__ to the label.\n",
    "    Args:\n",
    "        dataset_file(str) : path to dataset stored in CSV format\n",
    "\n",
    "    Returns:\n",
    "        train_df (pd.DataFrame) : training data\n",
    "        valid_df (pd.DataFrame) : validation data\n",
    "\n",
    "    each row in the dataframe would now look like\n",
    "    ```\n",
    "    |__label__234 |Cat toys, small, medium, large for all cats...|\n",
    "    ```\n",
    "    `|` denotes column separator\n",
    "    \"\"\"\n",
    "    print(\"Beginning Preprocessing ...\")\n",
    "    dataframe = preprocess(dataset_file)\n",
    "    print(\"Preprocessing Done!\")\n",
    "    training_df, test_df = train_test_split(dataframe, test_size=cfg.test_size)\n",
    "    train_df, valid_df = train_test_split(training_df, test_size=cfg.test_size)\n",
    "\n",
    "    del training_df\n",
    "\n",
    "    train_df.iloc[:, 0] = train_df.iloc[:, 0].progress_apply(lambda x: '__label__' + str(x))\n",
    "    valid_df.iloc[:, 0] = valid_df.iloc[:, 0].progress_apply(lambda x: '__label__' + str(x))\n",
    "    test_df.iloc[:, 0] = test_df.iloc[:, 0].progress_apply(lambda x: '__label__' + str(x))\n",
    "\n",
    "    txt_saver_util('test.txt', test_df)\n",
    "\n",
    "    return train_df, valid_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tokenization\n",
    "A custom BytePair Encoder is trained on the dataset corpus. Training a fasttext model on a custom tokenized dataset showed better performance than use the default fasttext encoding."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def tokenize_data():\n",
    "    \"\"\" Tokenizes Training and Validation data with a BPE (BytePair Encoding) Tokenizer \"\"\"\n",
    "    train_df, valid_df = create_splits(cfg.data_path)\n",
    "\n",
    "    def create_word_corpus(t_df:pd.DataFrame, v_df:pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Crates a word corpus to train the custom tokenizer on\n",
    "        Args:\n",
    "            t_df(pd.DataFrame) : training dataframe\n",
    "            v_df(pd.DataFrame) : validation dataframe\n",
    "        \"\"\"\n",
    "        corpus_df = pd.concat([t_df[['TEXT']],v_df[['TEXT']]])\n",
    "        txt_saver_util(cfg.corpus_file,corpus_df)\n",
    "\n",
    "    print(\"Creating corpus for tokenization ....\")\n",
    "    create_word_corpus(train_df, valid_df)\n",
    "    print(\"Done creating word corpus !\")\n",
    "\n",
    "    custom_tokenizer = ByteLevelBPETokenizer(lowercase=True)\n",
    "    print(\"Training Tokenizer ...\")\n",
    "    custom_tokenizer.train(cfg.corpus_file) # trains the tokenizer\n",
    "    custom_tokenizer.save(cfg.tokenizer)\n",
    "    print(\"Done creating tokenizer. Tokenizer saved as\",cfg.tokenizer)\n",
    "\n",
    "    def tokenize(training_df:pd.DataFrame, validation_df:pd.DataFrame, tokenizer):\n",
    "        \"\"\"\n",
    "        Tokenizes the \"TEXT\" column of the dataframe\n",
    "        Args:\n",
    "            training_df(pd.DataFrame) : training Data\n",
    "            validation_df(pd.DataFrame) : validation Data\n",
    "            tokenizer(tokenizer.ByteLevelBPETokenizer) : ByteLevelBPETokenizer\n",
    "        \"\"\"\n",
    "        training_df['TOKENIZED_TEXT'] = training_df['TEXT'].progress_apply(\n",
    "            lambda text: \" \".join(tokenizer.encode(text).tokens)\n",
    "        )\n",
    "        txt_saver_util('tokenized-train.txt',\n",
    "            training_df[['TOKENIZED_TEXT', 'BROWSE_NODE_ID']])\n",
    "\n",
    "        validation_df['TOKENIZED_TEXT'] = validation_df['TEXT'].progress_apply(\n",
    "            lambda text: \" \".join(tokenizer.encode(text).tokens)\n",
    "        )\n",
    "        txt_saver_util('tokenized-valid.txt',\n",
    "            validation_df[['TOKENIZED_TEXT', 'BROWSE_NODE_ID']])\n",
    "    print(\"Tokenizing data ...\")\n",
    "    tokenize(train_df, valid_df, custom_tokenizer)\n",
    "    print(\"Done tokenizing data!\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training and Inference\n",
    "The model in trained and its hyperparameters are autotuned for 30 minutes. The model size is limited to 100MiB as this is a restriction enforced by GitHub on the size of the trained model. A better performing model can be obtained by just removing the `autotuneModelSize` parameter. A hierarchical softmax is used as the loss function over the traditional softmax as it is much faster to train (8x in this case)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def train():\n",
    "    \"\"\"\n",
    "    Trains the FastText model with tokenized data. Model hyper-parameters are tuned and\n",
    "    the model size is limited to 99MB (GitHub Restriction)\n",
    "    \"\"\"\n",
    "    tokenize_data()\n",
    "\n",
    "    model = fasttext.train_supervised(\n",
    "    input='tokenized-train.txt',\n",
    "    wordNgrams = cfg.wordNgrams,\n",
    "    loss=cfg.loss,\n",
    "    epoch=cfg.epochs,\n",
    "    autotuneValidationFile='tokenized-valid.txt',\n",
    "    autotuneDuration= 1800,\n",
    "    autotuneModelSize=\"99M\")\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Preprocessing ...\n",
      "Preprocessing Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1709574 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3eb8668c34ca441a98ca9894c78a27c5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/189953 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "53287268ae1140d7bcff0b71cd298694"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/211059 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8fffb087c10e43b392737aabf2c4602d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating corpus for tokenization ....\n",
      "Done creating word corpus !\n",
      "Training Tokenizer ...\n",
      "\n",
      "\n",
      "\n",
      "Done creating tokenizer. Tokenizer saved as tokenizer.json\n",
      "Tokenizing data ...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1709574 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f697d82d53154f40a34dd8abc9989e02"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/189953 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2798b11e09af412ca47bf091d5946f49"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done tokenizing data!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : wordNgrams is manually set to a specific value. It will not be automatically optimized.\n",
      "Warning : loss is manually set to a specific value. It will not be automatically optimized.\n",
      "Warning : epoch is manually set to a specific value. It will not be automatically optimized.\n",
      "Progress: 100.0% Trials:    2 Best score:  0.684714 ETA:   0h 0m 0s\n",
      "Training again with best arguments\n",
      "Read 418M words\n",
      "Number of words:  29862\n",
      "Number of labels: 9509\n",
      "Progress: 100.0% words/sec/thread: 3421756 lr:  0.000000 avg.loss:  0.950186 ETA:   0h 0m 0s\n",
      "Progress: 100.0% words/sec/thread: 2778666 lr:  0.000000 avg.loss:  0.299240 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "trained_model = train()\n",
    "trained_model.save_model(cfg.model_file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "(189869, 0.6915346897071138, 0.6915346897071138)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model.test('tokenized-valid.txt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('__label__525',), array([0.43831539]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "loaded_tokenizer = Tokenizer.from_file(cfg.tokenizer)\n",
    "loaded_model = fasttext.load_model(cfg.model_file)\n",
    "\n",
    "def predict(text, tokenizer, model):\n",
    "    tokenized_text = ' '.join(tokenizer.encode(text).tokens)\n",
    "    return model.predict(tokenized_text)\n",
    "\n",
    "text = 'Pujyadeep  Puja  Agarbatti  (A  Box  Containing  12  Packets)  (Rooh  A  Gulab)  ' \\\n",
    "       'One  box  contains  twelve  packets  of  Incense  Sticks.  Very  Good  Fragrance. ' \\\n",
    "       ' Alight  this  incense  to  offer  your  deepest  devotion  and  open  your  heart  ' \\\n",
    "       'to  the  divine.  Made  In  India'\n",
    "\n",
    "print(predict(text, loaded_tokenizer, loaded_model))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}