{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import fasttext\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Tuple\n",
    "import pandas as pd\n",
    "import ml_collections\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"]=\"True\"\n",
    "tqdm.pandas()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def model_config():\n",
    "    cfg_dictionary = {\n",
    "        \"data_path\": \"dataset/train.csv\",\n",
    "\n",
    "        \"corpus_file\":\"word-corpus.txt\",\n",
    "        \"model_file\":\"fasttext-model.ftz\",\n",
    "\n",
    "        \"wordNgrams\":2,\n",
    "        \"test_size\": 0.1,\n",
    "        \"validation_size\":0.2,\n",
    "        \"loss\":'hs',\n",
    "        \"epochs\": 30,\n",
    "    }\n",
    "    config = ml_collections.FrozenConfigDict(cfg_dictionary)\n",
    "\n",
    "    return config\n",
    "cfg = model_config()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def txt_saver_util(file_path:str,df:pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Saves Dataframe to text (.txt) file\n",
    "    Args:\n",
    "        file_path(str) : path to where the generated file is to be saved\n",
    "        df(pd.DataFrame) : dataframe to be saved.\n",
    "\n",
    "    \"\"\"\n",
    "    df.to_csv(file_path,\n",
    "                    index=False,\n",
    "                    sep=' ',\n",
    "                    header=False,\n",
    "                    quoting=csv.QUOTE_NONE,\n",
    "                    quotechar=\"\",\n",
    "                    escapechar=\" \")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def preprocess(csv_path:str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads the csv, drops missing rows and concatenates columns and returns a dataframe\n",
    "    Args:\n",
    "        csv_path(str) : path the dataset file\n",
    "\n",
    "    Returns:\n",
    "        df(pd.DataFrame) : Preprocessed dataframe\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path,escapechar=\"\\\\\", quoting=3)\n",
    "    df = df.dropna()\n",
    "    df['TEXT'] = df['TITLE']+\" \"+df['DESCRIPTION']+\" \"+df[\"BULLET_POINTS\"]#+\" \"+df[\"BRAND\"]\n",
    "    df = df.drop(['TITLE','DESCRIPTION','BULLET_POINTS','BRAND'],axis=1)\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def create_splits(dataset_file:str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Creates train, validation, test split from the dataset.\n",
    "    Converts dataframe to a format that can be consumed by fasttext, by prepending __label__ to the label.\n",
    "    Args:\n",
    "        dataset_file(str) : path to dataset stored in CSV format\n",
    "\n",
    "    Returns:\n",
    "        train_df (pd.DataFrame) : training data\n",
    "        valid_df (pd.DataFrame) : validation data\n",
    "\n",
    "    each row in the dataframe would now look like\n",
    "    ```\n",
    "    |__label__234 |Cat toys, small, medium, large for all cats...|\n",
    "    ```\n",
    "    `|` denotes column separator\n",
    "    \"\"\"\n",
    "    print(\"Beginning Preprocessing ...\")\n",
    "    dataframe = preprocess(dataset_file)\n",
    "    print(\"Preprocessing Done!\")\n",
    "    training_df, test_df = train_test_split(dataframe, test_size=cfg.test_size)\n",
    "    train_df, valid_df = train_test_split(training_df, test_size=cfg.test_size)\n",
    "\n",
    "    del training_df\n",
    "\n",
    "    train_df.iloc[:, 0] = train_df.iloc[:, 0].apply(lambda x: '__label__' + str(x))\n",
    "    valid_df.iloc[:, 0] = valid_df.iloc[:, 0].apply(lambda x: '__label__' + str(x))\n",
    "    test_df.iloc[:, 0] = test_df.iloc[:, 0].apply(lambda x: '__label__' + str(x))\n",
    "\n",
    "    txt_saver_util('test.txt', test_df)\n",
    "\n",
    "    return train_df, valid_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def tokenize_data():\n",
    "    \"\"\" Tokenizes Training and Validation data with a BPE (BytePair Encoding) Tokenizer \"\"\"\n",
    "    train_df, valid_df = create_splits(cfg.data_path)\n",
    "\n",
    "    def create_word_corpus(t_df:pd.DataFrame, v_df:pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Crates a word corpus to train the custom tokenizer on\n",
    "        Args:\n",
    "            t_df(pd.DataFrame) : training dataframe\n",
    "            v_df(pd.DataFrame) : validation dataframe\n",
    "        \"\"\"\n",
    "        corpus_df = pd.concat([t_df[['TEXT']],v_df[['TEXT']]])\n",
    "        txt_saver_util(cfg.corpus_file,corpus_df)\n",
    "\n",
    "    print(\"Creating corpus for tokenization ....\")\n",
    "    create_word_corpus(train_df, valid_df)\n",
    "    print(\"Done creating word corpus !\")\n",
    "\n",
    "    custom_tokenizer = ByteLevelBPETokenizer(lowercase=True)\n",
    "    print(\"Training Tokenizer ...\")\n",
    "    custom_tokenizer.train(cfg.corpus_file) # trains the tokenizer\n",
    "    custom_tokenizer.save('tokenizer.json')\n",
    "    print(\"Done creating tokenizer. Tokenizer saved as `tokenizer.json`\")\n",
    "\n",
    "    def tokenize(training_df:pd.DataFrame, validation_df:pd.DataFrame, tokenizer):\n",
    "        \"\"\"\n",
    "        Tokenizes the \"TEXT\" column of the dataframe\n",
    "        Args:\n",
    "            training_df(pd.DataFrame) : training Data\n",
    "            validation_df(pd.DataFrame) : validation Data\n",
    "            tokenizer(tokenizer.ByteLevelBPETokenizer) : ByteLevelBPETokenizer\n",
    "        \"\"\"\n",
    "        training_df['TOKENIZED_TEXT'] = training_df['TEXT'].progress_apply(\n",
    "            lambda text: \" \".join(tokenizer.encode(text).tokens)\n",
    "        )\n",
    "        txt_saver_util('tokenized-train.txt',\n",
    "            training_df[['TOKENIZED_TEXT', 'BROWSE_NODE_ID']])\n",
    "\n",
    "        validation_df['TOKENIZED_TEXT'] = validation_df['TEXT'].progress_apply(\n",
    "            lambda text: \" \".join(tokenizer.encode(text).tokens)\n",
    "        )\n",
    "        txt_saver_util('tokenized-valid.txt',\n",
    "            validation_df[['TOKENIZED_TEXT', 'BROWSE_NODE_ID']])\n",
    "    print(\"Tokenizing data ...\")\n",
    "    tokenize(train_df, valid_df, custom_tokenizer)\n",
    "    print(\"Done tokenizing data!\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def train():\n",
    "    \"\"\"\n",
    "    Trains the FastText model with tokenized data. Model hyper-parameters are tuned and\n",
    "    the model size is limited to 99MB (GitHub Restriction)\n",
    "    \"\"\"\n",
    "    tokenize_data()\n",
    "\n",
    "    model = fasttext.train_supervised(\n",
    "    input='tokenized-train.txt',\n",
    "    wordNgrams = cfg.wordNgrams,\n",
    "    loss=cfg.loss,\n",
    "    epoch=cfg.epochs,\n",
    "    autotuneValidationFile='tokenized-valid.txt',\n",
    "    autotuneDuration= 1800,\n",
    "    autotuneModelSize=\"99M\")\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Preprocessing ...\n",
      "Preprocessing Done!\n",
      "Creating corpus for tokenization ....\n",
      "Done creating word corpus !\n",
      "Training Tokenizer ...\n",
      "\n",
      "\n",
      "\n",
      "Done creating tokenizer. Tokenizer saved as `tokenizer.json`\n",
      "Tokenizing data ...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1709574 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "af06504a050b49fab91d53eb21b03bee"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/189953 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5778c08fd88c4262ad96b8704edb4822"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done tokenizing data!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : wordNgrams is manually set to a specific value. It will not be automatically optimized.\n",
      "Warning : loss is manually set to a specific value. It will not be automatically optimized.\n",
      "Warning : epoch is manually set to a specific value. It will not be automatically optimized.\n",
      "Progress: 100.0% Trials:    3 Best score:  0.678112 ETA:   0h 0m 0s\n",
      "Training again with best arguments\n",
      "Read 414M words\n",
      "Number of words:  29867\n",
      "Number of labels: 9512\n",
      "Progress: 100.0% words/sec/thread: 3276624 lr:  0.000000 avg.loss:  0.912127 ETA:   0h 0m 0s\n",
      "Progress: 100.0% words/sec/thread: 2662359 lr:  0.000000 avg.loss:  0.301117 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "trained_model = train()\n",
    "trained_model.save_model(cfg.model_file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "(189883, 0.6945329492371618, 0.6945329492371618)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model.test('tokenized-valid.txt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}